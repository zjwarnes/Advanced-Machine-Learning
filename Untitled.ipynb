{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from cmath import sqrt\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import random\n",
    "import math\n",
    "import re, string\n",
    "##########################################\n",
    "## HyperParameters\n",
    "# Number of hidden layers and Nodes\n",
    "# learning rate\n",
    "# momentum\n",
    "##########################################\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "trainX = np.load('tinyX.npy') # this should have shape (26344, 3, 64, 64)\n",
    "trainY = np.load('tinyY.npy') \n",
    "testX = np.load('tinyX_test.npy') # (6600, 3, 64, 64)\n",
    "\n",
    "sample_count = 26344\n",
    "class_count = 40\n",
    "grey_size = 64*64+1;\n",
    "\n",
    "\n",
    "\n",
    "trainY_results=np.load('trainY_flat.npy')\n",
    "print(trainY_results.shape)\n",
    "gray_train = np.load('gray_flat_train.npy')\n",
    "print(gray_train.shape)\n",
    "gray_test = np.load('gray_flat_test.npy')\n",
    "print(gray_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Shuffle the Input and Outputs\n",
    "training_set_index = np.arange(sample_count)\n",
    "np.random.shuffle(training_set_index)\n",
    "\n",
    "gray_train2= np.zeros((sample_count,64*64+1))\n",
    "trainY_results2 = np.zeros((sample_count,40))\n",
    "for i in range(sample_count):\n",
    "    shuffle_i = training_set_index[i]\n",
    "    gray_train2[i,:]=gray_train[shuffle_i]\n",
    "    trainY_results2[i,:]=trainY_results[shuffle_i]\n",
    "\n",
    "#Training And Validation\n",
    "validation_size=500\n",
    "training_size = sample_count-validation_size\n",
    "gray_valid = gray_train2[training_size:sample_count,:]\n",
    "trainY_valid = trainY_results2[training_size:sample_count,:]\t\n",
    "\n",
    "\n",
    "## Neural Network\n",
    "total_epoch_error=100\n",
    "max_epoch=3\n",
    "epoch_results = np.zeros((max_epoch,3))\n",
    "alpha = 0.01\n",
    "#Random Value Weights\n",
    "hl_weights=np.random.random((40,grey_size))/100\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    total_error=0\n",
    "    for sample_index in range(training_size):\n",
    "        sig1 = np.zeros(40)\n",
    "        output = np.ones(40)\n",
    "        #\n",
    "        for i in range(40):\n",
    "            output[i] = 1/(1+np.exp(-np.dot(gray_train2[sample_index,:],hl_weights[i,:])))\n",
    "\n",
    "        output = softmax(output)\n",
    "        error=0\n",
    "        for i in range(class_count):\n",
    "            error=error+0.5*((trainY_results2[sample_index,i]-output[i])**2)\n",
    "        # print(error)\n",
    "        total_error=total_error+error\n",
    "\n",
    "        #Weight Update\n",
    "        for j in range(40):\n",
    "            sig1[j]=-1*(trainY_results2[sample_index,j]-output[j])*output[j]*(1-output[j])\n",
    "            for i in range(grey_size):\n",
    "                hl_weights[j,i] = hl_weights[j,i]-alpha*sig1[j]*gray_train2[sample_index,i]\t\t\n",
    "\n",
    "\n",
    "        if(sample_index%500==0):\n",
    "            print(\"At Index: \", sample_index,\" and epoch: \", epoch)\n",
    "\n",
    "\n",
    "\n",
    "    validation_error = 0\n",
    "    correct_predictions=0\n",
    "    for sample_index in range(validation_size):\n",
    "\n",
    "        output = np.ones(40)\n",
    "\n",
    "        for i in range(40):\n",
    "            output[i] = 1/(1+np.exp(-np.dot(gray_valid[sample_index,:],hl_weights[i,:])))\n",
    "\n",
    "\n",
    "        output = softmax(output)\n",
    "        if np.isnan(np.max(output))==1:\n",
    "            print(\"Big Problem\")\n",
    "\n",
    "        error=0\n",
    "        for i in range(class_count):\n",
    "            error=error+0.5*((trainY_valid[sample_index,i]-output[i])**2)\t\n",
    "        validation_error = validation_error + error\n",
    "\n",
    "        if output.argmax(axis=0) == trainY_valid[i,:].argmax(axis=0):\n",
    "            correct_predictions = correct_predictions+1\n",
    "\n",
    "    print(\"Correct Predictions: \", correct_predictions)\t\n",
    "    print(\"Accuracy: \", correct_predictions/validation_size)\n",
    "    epoch_results[epoch,:]=[epoch,total_error,validation_error]\n",
    "    np.save('Weights.npy', hl_weights)\n",
    "    print(\"Epoch Count: \", epoch, \" with error \", total_error, \" and validation error\", validation_error)\n",
    "    if total_error<total_epoch_error:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "## Show Image\n",
    "###\n",
    "# index=10000\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(trainX[index].transpose(2,1,0))\n",
    "# plt.show()\n",
    "# print(trainY[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# load data: data loaded when .py file is in same directory as the .npy files\n",
    "print('Loading data...')\n",
    "\n",
    "X_train = np.load('tinyX.npy')\n",
    "print('X_train shape is: ', X_train.shape)\n",
    "print('X_train shape is: ', X_train.dtype)\n",
    "# no need ##X_train = X_train.flatten().reshape(26344, 3*64*64)\n",
    "print('X_train shape is: ', X_train.shape)\n",
    "print('X_train shape is: ', X_train.dtype)\n",
    "\n",
    "\n",
    "Y_train = np.load('tinyY.npy')\n",
    "print('Y_train shape is: ', Y_train.shape)\n",
    "print('Y_train shape is: ', Y_train.dtype)\n",
    "# no need ##Y_train = Y_train.flatten()\n",
    "print('Y_train shape is: ', Y_train.shape)\n",
    "print('Y_train shape is: ', Y_train.dtype)\n",
    "\n",
    "#convert the training output into a 40-col matrix\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "number_classes = Y_train.shape[1]\n",
    "\n",
    "X_test = np.load('tinyX_test.npy')\n",
    "print('X_test shape is: ', X_test.shape)\n",
    "print('X_test shape is: ', X_test.dtype)\n",
    "# no need ##X_test = X_test.flatten().reshape(6600, 3*64*64)\n",
    "print('X_test shape is: ', X_test.shape)\n",
    "print('X_test shape is: ', X_test.dtype)\n",
    "\n",
    "print('')\n",
    "\n",
    "# convert the inputs, which are integers, to floats\n",
    "# then, normalize the inputs, which are RGB ranging from 0-255, to 0-1\n",
    "print('Normalizing input data...')\n",
    "X_train = X_train.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "print('X_train shape is: ', X_train.shape)\n",
    "print('X_train shape is: ', X_train.dtype)\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test = X_test / 255.0\n",
    "print('X_test shape is: ', X_test.shape)\n",
    "print('X_test shape is: ', X_test.dtype)\n",
    "\n",
    "print('')\n",
    "\n",
    "# visualizing\n",
    "print(X_train[1])\n",
    "print('=========')\n",
    "print(X_train[1].transpose())\n",
    "print('=========')\n",
    "\n",
    "# create model\n",
    "# add dropoout throughout to reduce overfitting\n",
    "print('Creating model...')\n",
    "model = Sequential()\n",
    "\n",
    "# filters = neurons\n",
    "model.add(Convolution2D(64, 3, 3, input_shape=(3, 64, 64), border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same', W_constraint=maxnorm(3)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))       # generalize the feature maps generated from the previous 2 convolutions\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# 2 hidden layers, 32 neurons each, due to limitations\n",
    "model.add(Dense(32, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(number_classes, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "# initialize variables for SGD\n",
    "learning_rate = 0.01    # arbitrary\n",
    "epochs = 10             # due to time constraints\n",
    "decay = learning_rate/epochs    \n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=decay, nesterov=False)  # stochastic gradient descent\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# print summary\n",
    "print(model.summary())\n",
    "\n",
    "# fit model\n",
    "print('Fitting...')\n",
    "model.fit(X_train, Y_train, nb_epoch=epochs, batch_size=100, verbose=2) # update every 100 images\n",
    "\n",
    "# predict\n",
    "print('Predicting...')\n",
    "predict = model.predict(X_test)\n",
    "\n",
    "# print prediction\n",
    "print(predict)\n",
    "\n",
    "print('Finished prediction. Need to create the .csv file.')\n",
    "\n",
    "\n",
    "# creating the output file\n",
    "# predict 40-col arrays data\n",
    "file = open(\"results.csv\",\"w\") \n",
    "file.write(\"id,class\\n\")\n",
    "\n",
    "for i in range(len(predict)):\n",
    "\tfile.write(str(i) + \",\" + str(predict[i]) + \"\\n\")\n",
    "file.close() \n",
    "\n",
    "print('.csv file created.')\n",
    "print('END')\n",
    "\n",
    "# prediction class data\n",
    "file2 = open(\"results2.csv\", \"w\")       # submit this file\n",
    "file2.write(\"id,class\\n\")\n",
    "\n",
    "for i in range(len(predict)):\n",
    "\tfile2.write(str(i) + \",\" + str(predict[i].argmax(axis=0)) + \"\\n\")     # finds index of the max element in array\n",
    "file.close() \n",
    "\n",
    "print('.csv file created.')\n",
    "print('END')\n",
    "# END #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
