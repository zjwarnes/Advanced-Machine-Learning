{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "This network does not always converge towards an optimal solution. Due to random initialization of weights and selected value for alpha, the network may produce strange results. Many of the variations for the aspects described above produced accurate networks. Ultimately, a learning rate of 1 was used with a 4 set as the random seed, 60 000 iterations and stopping criteria set to 1e-15. This formation consistently provides good results as well as weights and activations that illustrate the underlying properties of the network in a relatively straightforward manner. Training the network takes less than 20 seconds depending on the computer used and the cross-entrophy cost after the final iteratiion is 0.016892. Note the weights and activation values shown are rounded to the third decimal place so that they are easier to read. The tables below are based on the setup described, variations to the model would produce different results.\n",
    "\n",
    "\n",
    "## Weights\n",
    "\n",
    "The first striking aspect of this neural network is that despite simple input and outputs the scale of the weights is quite high. The first layer contains weights with values in the 1000s and the subsequent hidden layer has reduced weights in the 10s range. The degree to this first layer of weights appears to be related to the degree of accuracy of the final activation. The large the weight, the larger the separation between different inputs and their corresponding outputs. In the initial layer, the weight for a particular neuron is only, used when multiplied by the binary vector with the same entry. However in the hidden layer, the weights are now interconnected between all the input instances therefore, extreme values would greatly vary the activation into the final layer.\n",
    "\n",
    "\n",
    "## Activation Values\n",
    "\n",
    "Activation values for Layer 1:\n",
    "\n",
    "|   |0\t    |1\t    |2\t    |3   \t|4   \t|5   \t|6   \t|7    |\n",
    "|---|-------|-------|-------|-------|-------|-------|-------|-----|\n",
    "|0\t|0.440\t|0.000\t|0.000\t|0.439\t|0.466\t|0.000\t|0.000\t|0.394|\n",
    "|1\t|0.000\t|0.537\t|0.340\t|0.474\t|0.405\t|0.000\t|0.116\t|0.000|\n",
    "|2\t|0.418\t|0.095\t|0.532\t|0.449\t|0.000\t|0.334\t|0.000\t|0.000|\n",
    "\n",
    "\n",
    "The activation values for each layer are far more interesting. Due to the nature of the inputs and desired outputs this network is an autoencoder. This type of network acts as the identify functions for a set of inputs. More importantly, it can be used to determine a compressed representation of the data preserving only necessary information. From the activations of layer 1 above we can see this at work. At first, these values appear all over the place but once the columns are ordered (6, 5, 1, 2, 7, 0, 4, 3) a pattern emerges. If the near-zero values are set to zero for these vectors and the non-zero values set to 1 the binary numbers 0 through to 7 emerge (see the table below, assume near-zero is < 0.12). This sequence contains the same information as our input (the number 1 shifted amongst 8 positions) in the least amount of information possible. The dimensions of our input 8 x 8 can now be represented in using only 3 x 8 dimensions while maintaining the desired information. This neural network acts as a mapping between the initial representation and the compressed form. \n",
    "\n",
    "\n",
    "|   |6\t    |5\t    |1\t    |2   \t|7   \t|0   \t|4   \t|3    |\n",
    "|---|-------|-------|-------|-------|-------|-------|-------|-----|\n",
    "|0\t|0.000\t|0.000\t|0.000\t|0.000\t|0.394\t|0.440\t|0.466\t|0.439|\n",
    "|1\t|0.116\t|0.000\t|0.537\t|0.340\t|0.000\t|0.000\t|0.405\t|0.474|\n",
    "|2\t|0.000\t|0.334\t|0.095\t|0.532\t|0.000\t|0.418\t|0.000\t|0.449|\n",
    "\n",
    "The final output reveals just how close the values are to reproducing the input. However, what is also clear is that many values are not exactly 0 and not exactly 1. This indicates that this particular network does not produce the global optimum value and can be further trained and improved. This improvement should also reinforce the properties described in the first activation layer.\n",
    "\n",
    "-Final Activation Layer:\n",
    "\n",
    "|   |0\t    |1\t    |2\t    |3   \t|4   \t|5   \t|6   \t|7    |\n",
    "|---|-------|-------|-------|-------|-------|-------|-------|-----|\n",
    "|0\t|0.994\t|0.000\t|0.000\t|0.002\t|0.000\t|0.001\t|0.000\t|0.002|\n",
    "|1\t|0.000\t|0.994\t|0.002\t|0.000\t|0.002\t|0.000\t|0.002\t|0.000|\n",
    "|2\t|0.000\t|0.002\t|0.992\t|0.002\t|0.000\t|0.004\t|0.000\t|0.000|\n",
    "|3\t|0.002\t|0.000\t|0.003\t|0.992\t|0.003\t|0.000\t|0.000\t|0.000|\n",
    "|4\t|0.000\t|0.002\t|0.000\t|0.003\t|0.991\t|0.000\t|0.000\t|0.004|\n",
    "|5\t|0.002\t|0.000\t|0.004\t|0.000\t|0.000\t|0.988\t|0.006\t|0.000|\n",
    "|6\t|0.000\t|0.004\t|0.000\t|0.000\t|0.000\t|0.002\t|0.990\t|0.004|\n",
    "|7\t|0.001\t|0.000\t|0.000\t|0.000\t|0.003\t|0.000\t|0.004\t|0.992|\n",
    "\n",
    "\n",
    "## Detailed Code Below:\n",
    " - Run each cell in order\n",
    " - The final cell trains the network and displays results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, hidden_layer_size = 3):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = np.size(X,axis=0) # size of input layer\n",
    "    n_h = hidden_layer_size\n",
    "    n_y = np.size(Y, axis=0) # size of output layer\n",
    "    \n",
    "    return (n_x, n_h, n_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    return np.power( 1 + np.exp( -z ), -1)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax values for the output layer\n",
    "    \n",
    "    Arguments:\n",
    "    z -- A numpy array of any size\n",
    "    \n",
    "    Return:\n",
    "    -- softmax(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ex = np.exp( z - np.max(z))\n",
    "    return ex / ex.sum( axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing parameters:\n",
    "        W1 -- weight matrix of shape (n_h, n_x)\n",
    "        b1 -- bias vector of shape (n_h, 1)\n",
    "        W2 -- weight matrix of shape (n_y, n_h)\n",
    "        b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INITIALIZE WEIGHTS AND BIAS FOR EACH LAYER\n",
    "    W1 = np.random.randn( n_h * n_x).reshape( n_h, n_x) * 0.01\n",
    "    b1 = np.zeros( n_h).reshape( n_h, 1) * 0.01\n",
    "    W2 = np.random.randn( n_y * n_h).reshape( n_y, n_h) * 0.01\n",
    "    b2 = np.zeros( n_y).reshape( n_y, 1) * 0.01\n",
    "    \n",
    "    \n",
    "    # ASSERT DIMENSIONS ARE CORRECT\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activation = 'sigmoid', output_type = 'binary'):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing parameters \n",
    "    (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    \n",
    "    \n",
    "    # LAYER ONE\n",
    "    Z1 = np.dot( W1, X) + b1\n",
    "    \n",
    "    if activation == 'tanh':\n",
    "        A1 = np.tanh( Z1)\n",
    "    else:\n",
    "        A1 = sigmoid( Z1)\n",
    "    \n",
    "    # LAYER TWO\n",
    "    Z2 = np.dot( W2, A1) + b2\n",
    "    \n",
    "    if output_type == 'multi':\n",
    "        A2 = softmax( Z2)\n",
    "    else:\n",
    "        if activation == 'tanh':\n",
    "            A2 = np.tanh( Z2)\n",
    "        else:\n",
    "            A2 = sigmoid( Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = (A2 - Y)\n",
    "    dW2 = ( 1 / m) * np.dot( dZ2, A1.T)\n",
    "    db2 = ( 1 / m) * np.sum( dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot( W2.T, dZ2) * ( 1 - np.power( A1, 2))\n",
    "    #dZ1 = np.dot( W2.T, dZ2) * ( 1 - A1)\n",
    "    dW1 = ( 1 / m) * np.dot( dZ1, X.T)\n",
    "    db1 = ( 1 / m) * np.sum( dZ1, axis =1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def backward_propagation_regularization(parameters, cache, X, Y, lmbda = 1.0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    lmbda -- hyperparameter to give weight to the regularization term\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = ( 1 / m) * np.dot( dZ2, A1.T) + (lmbda * W2) / m\n",
    "    db2 = ( 1 / m) * np.sum( dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot( W2.T, dZ2) * ( 1 - np.power( A1, 2))\n",
    "    dW1 = ( 1 / m) * np.dot( dZ1, X.T)  + (lmbda * W1) / m\n",
    "    db1 = ( 1 / m) * np.sum( dZ1, axis =1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    grads -- python dictionary containing gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply( np.log( A2), Y) + np.multiply( np.log(1-A2), (1-Y))\n",
    "    \n",
    "    cost = -( 1 / m) * np.sum( logprobs)\n",
    "    \n",
    "    # CONFIRM DIMENSIONS\n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def compute_cost_regularization(A2, Y, parameters, lmbda = 1.0):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    lmbda -- hyperparameter to give weight to the regularization term\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply( np.log( A2), Y) + np.multiply( np.log(1-A2), (1-Y))\n",
    "    reg_cost = (np.sum( np.square(W1)) + np.sum( np.square(W2)))\n",
    "    cost = -( 1 / m) * np.sum( logprobs) + (lmbda / (2* m)) * reg_cost\n",
    "    \n",
    "    # CONFIRM DIMENSIONS\n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, num_iter = 10000, print_cost = False, output_type = 'binary', learning_rate = 1.2, epsilon = 1e-7, reg = False, lmbda = 1.0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. \n",
    "    # Inputs: \"n_x, n_h, n_y\". \n",
    "    # Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y) \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    cost_prev = 10000\n",
    "   \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iter):\n",
    "         \n",
    "        # Forward propagation. \n",
    "        # Inputs: \"X, parameters\". \n",
    "        # Outputs: \"A2, cache\".\n",
    "       \n",
    "        A2, cache = forward_propagation(X, parameters, output_type = output_type)\n",
    "        \n",
    "        # Cost function. \n",
    "        # Inputs: \"A2, Y, parameters\". \n",
    "        # Outputs: \"cost\".\n",
    "        if reg == True:\n",
    "            cost = compute_cost_regularization(A2,Y,parameters, lmbda)\n",
    "        else:\n",
    "            cost = compute_cost(A2,Y,parameters)\n",
    "     \n",
    "        # Backpropagation. \n",
    "        # Inputs: \"parameters, cache, X, Y\". \n",
    "        # Outputs: \"grads\".\n",
    "        if reg == True:\n",
    "            grads = backward_propagation_regularization(parameters, cache, X, Y, lmbda)\n",
    "        else: \n",
    "            grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. \n",
    "        # Inputs: \"parameters, grads\". \n",
    "        # Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "          \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 5000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if abs(cost_prev - cost) < epsilon and cost < 0.04:\n",
    "            print (\"Final cost after iteratiion %i: %f\" %(i, cost))\n",
    "            parameters['A1'] = cache['A1']\n",
    "            parameters['A2'] = cache['A2']\n",
    "            return parameters\n",
    "        else:\n",
    "            cost_prev = cost\n",
    "    \n",
    "    parameters['A1'] = cache['A1']\n",
    "    parameters['A2'] = cache['A2']\n",
    "    print (\"Final cost after iteratiion %i: %f\" %(num_iter, cost))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X, output_type = 'binary'):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation\n",
    "    A2, cache = forward_propagation( X, parameters, output_type = output_type)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Model\n",
    "'''\n",
    "np.random.seed(7)\n",
    "print(\"Regularized Model\")\n",
    "parameters_reg = model(\n",
    "    train_set_x, \n",
    "    train_set_x, \n",
    "    n_h = 3, \n",
    "    num_iter = 40000, \n",
    "    print_cost = True,\n",
    "    learning_rate = 1.0,\n",
    "    epsilon = 1e-10,\n",
    "    reg = True,\n",
    "    lmbda = 0.001\n",
    ")\n",
    "display( pd.DataFrame( predict( parameters_reg, train_set_x) * 1) )\n",
    "\n",
    "# Analysis of Weights for regularized model\n",
    "W1 = pd.DataFrame( parameters_reg['W1'])\n",
    "A1 = pd.DataFrame( parameters_reg['A1'])\n",
    "b1 = pd.DataFrame( parameters_reg['b1'])\n",
    "W2 = pd.DataFrame( parameters_reg['W2'])\n",
    "A2 = pd.DataFrame( parameters_reg['A2'])\n",
    "b2 = pd.DataFrame( parameters_reg['b2'])\n",
    "\n",
    "print(\"Reg. Weights Layer 1:\")\n",
    "display(np.round(W1,3))\n",
    "print(\"Reg. Activations Layer 1:\")\n",
    "display(np.round(A1,3))\n",
    "print(\"Reg. Bias Layer 1:\")\n",
    "display(np.round(b1,3))\n",
    "\n",
    "print(\"Reg. Weights Layer 2:\")\n",
    "display(np.round(W2,3))\n",
    "print(\"Reg. Activations Layer 2:\")\n",
    "display(np.round(A2,3))\n",
    "print(\"Reg. Bias Layer 2:\")\n",
    "display(np.round(b2,3))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.552610\n",
      "Cost after iteration 5000: 0.142670\n",
      "Cost after iteration 10000: 0.075613\n",
      "Cost after iteration 15000: 0.280704\n",
      "Cost after iteration 20000: 0.035790\n",
      "Cost after iteration 25000: 0.028745\n",
      "Cost after iteration 30000: 0.031650\n",
      "Cost after iteration 35000: 0.012350\n",
      "Cost after iteration 40000: 0.013594\n",
      "Cost after iteration 45000: 0.015203\n",
      "Cost after iteration 50000: 0.016312\n",
      "Cost after iteration 55000: 0.016864\n",
      "Final cost after iteratiion 60000: 0.016892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7\n",
       "0  1  0  0  0  0  0  0  0\n",
       "1  0  1  0  0  0  0  0  0\n",
       "2  0  0  1  0  0  0  0  0\n",
       "3  0  0  0  1  0  0  0  0\n",
       "4  0  0  0  0  1  0  0  0\n",
       "5  0  0  0  0  0  1  0  0\n",
       "6  0  0  0  0  0  0  1  0\n",
       "7  0  0  0  0  0  0  0  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Layer 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1091.841</td>\n",
       "      <td>-1414.086</td>\n",
       "      <td>-209.158</td>\n",
       "      <td>1091.839</td>\n",
       "      <td>1091.947</td>\n",
       "      <td>-3234.671</td>\n",
       "      <td>-601.468</td>\n",
       "      <td>1091.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-2000.182</td>\n",
       "      <td>1192.926</td>\n",
       "      <td>1192.115</td>\n",
       "      <td>1192.674</td>\n",
       "      <td>1192.394</td>\n",
       "      <td>-3767.683</td>\n",
       "      <td>1190.747</td>\n",
       "      <td>-1385.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1383.171</td>\n",
       "      <td>1381.246</td>\n",
       "      <td>1383.628</td>\n",
       "      <td>1383.295</td>\n",
       "      <td>-1185.740</td>\n",
       "      <td>1382.810</td>\n",
       "      <td>-4240.325</td>\n",
       "      <td>-2871.586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1091.841 -1414.086  -209.158  1091.839  1091.947 -3234.671  -601.468   \n",
       "1 -2000.182  1192.926  1192.115  1192.674  1192.394 -3767.683  1190.747   \n",
       "2  1383.171  1381.246  1383.628  1383.295 -1185.740  1382.810 -4240.325   \n",
       "\n",
       "          7  \n",
       "0  1091.653  \n",
       "1 -1385.775  \n",
       "2 -2871.586  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations Layer 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7\n",
       "0  0.440  0.000  0.000  0.439  0.466  0.000  0.000  0.394\n",
       "1  0.000  0.537  0.340  0.474  0.405  0.000  0.116  0.000\n",
       "2  0.418  0.095  0.532  0.449  0.000  0.334  0.000  0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Layer 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1092.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1192.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1383.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -1092.084\n",
       "1 -1192.779\n",
       "2 -1383.500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Layer 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>21.860</td>\n",
       "      <td>-24.872</td>\n",
       "      <td>24.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-18.852</td>\n",
       "      <td>29.302</td>\n",
       "      <td>-13.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-22.668</td>\n",
       "      <td>12.602</td>\n",
       "      <td>31.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>21.236</td>\n",
       "      <td>21.644</td>\n",
       "      <td>21.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>23.949</td>\n",
       "      <td>21.203</td>\n",
       "      <td>-25.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-27.739</td>\n",
       "      <td>-38.055</td>\n",
       "      <td>15.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-29.963</td>\n",
       "      <td>-15.186</td>\n",
       "      <td>-37.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>17.460</td>\n",
       "      <td>-29.539</td>\n",
       "      <td>-30.326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2\n",
       "0  21.860 -24.872  24.072\n",
       "1 -18.852  29.302 -13.033\n",
       "2 -22.668  12.602  31.251\n",
       "3  21.236  21.644  21.388\n",
       "4  23.949  21.203 -25.695\n",
       "5 -27.739 -38.055  15.408\n",
       "6 -29.963 -15.186 -37.975\n",
       "7  17.460 -29.539 -30.326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations Layer 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7\n",
       "0  0.994  0.000  0.000  0.002  0.000  0.001  0.000  0.002\n",
       "1  0.000  0.994  0.002  0.000  0.002  0.000  0.002  0.000\n",
       "2  0.000  0.002  0.992  0.002  0.000  0.004  0.000  0.000\n",
       "3  0.002  0.000  0.003  0.992  0.003  0.000  0.000  0.000\n",
       "4  0.000  0.002  0.000  0.003  0.991  0.000  0.000  0.004\n",
       "5  0.002  0.000  0.004  0.000  0.000  0.988  0.006  0.000\n",
       "6  0.000  0.004  0.000  0.000  0.000  0.002  0.990  0.004\n",
       "7  0.001  0.000  0.000  0.000  0.003  0.000  0.004  0.992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Layer 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-14.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-9.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-16.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-24.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-14.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-2.029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0 -14.626\n",
       "1  -9.393\n",
       "2 -16.060\n",
       "3 -24.415\n",
       "4 -14.999\n",
       "5  -0.707\n",
       "6   6.329\n",
       "7  -2.029"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "train_set_x = np.array(\n",
    "    [[1,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,1]]\n",
    ")\n",
    "\n",
    "\n",
    "np.random.seed(4)\n",
    "parameters = model(\n",
    "    train_set_x, \n",
    "    train_set_x, \n",
    "    n_h = 3, \n",
    "    num_iter = 60000, \n",
    "    print_cost = True,\n",
    "    learning_rate = 1.0,\n",
    "    epsilon = 1e-15,\n",
    "    reg = False\n",
    ")\n",
    "display( pd.DataFrame( predict( parameters, train_set_x) * 1) )\n",
    "\n",
    "# Analysis of Weights\n",
    "W1 = pd.DataFrame( parameters['W1'])\n",
    "A1 = pd.DataFrame( parameters['A1'])\n",
    "b1 = pd.DataFrame( parameters['b1'])\n",
    "W2 = pd.DataFrame( parameters['W2'])\n",
    "A2 = pd.DataFrame( parameters['A2'])\n",
    "b2 = pd.DataFrame( parameters['b2'])\n",
    "\n",
    "\n",
    "# Values are rounded, for easy readability\n",
    "print(\"Weights Layer 1:\")\n",
    "display(np.round(W1,3))\n",
    "print(\"Activations Layer 1:\")\n",
    "display(np.round(A1,3))\n",
    "print(\"Bias Layer 1:\")\n",
    "display(np.round(b1,3))\n",
    "\n",
    "print(\"Weights Layer 2:\")\n",
    "display(np.round(W2,3))\n",
    "print(\"Activations Layer 2:\")\n",
    "display(np.round(A2,3))\n",
    "print(\"Bias Layer 2:\")\n",
    "display(np.round(b2,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
