{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from surprise import NormalPredictor, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "from surprise import SVD, SVDpp, NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise.reader import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "cv_num = 5\n",
    "measure_set = ['RMSE', 'MSE', 'MAE']\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "db = \"UCM\"\n",
    "table = \"dbo.[student_course_dataset]\"\n",
    "\n",
    "query = \"SELECT * FROM \" + db + \".\" + table\n",
    "\n",
    "conn = pyodbc.connect(\n",
    "    'Driver={SQL Server};'\n",
    "    'Server=DESKTOP-8LSE8HT;'\n",
    "    'Database=UCM;'\n",
    "    'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "reader = Reader()\n",
    "df = Dataset.load_from_df(df, reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Predictor\n",
    "Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be NORMAL.\n",
    "\n",
    "The prediction $\\hat{r}_{ui}$ is generated from a normal distribution with sample mean and sample deviation are calculated using Maximum Likelihood Estimation.\n",
    "\n",
    " * $\\hat{\\mu} = \\frac{1}{|R_{train}|} \\sum_{r_{ui}\\epsilon R_{train}}r_{ui}$\n",
    " \n",
    " * $\\hat{\\sigma}^2 = \\sum_{r_{ui} \\epsilon R_{train} } \\frac{(r_{ui} - \\hat{\\mu})^2}{|R_{train}|}$\n",
    " \n",
    " \n",
    "For prediction on a given user we have $\\hat{r_{ui}} = b_{ui} = \\mu + b_u + b_i$, with user u, bias $b_u$ and item $b_i$ for an unknown user $b_u$ is assumed to be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normal Predictor\n",
    "rs = NormalPredictor()\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num \n",
    "    #,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN Algorithms\n",
    "\n",
    "### Base k-NN Algorithm\n",
    "Predictions $\\hat{r_{ui}}$ are set with:\n",
    "\n",
    " * $\\hat{r_{ui}} = \\frac{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) * r_{vi} }{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) }$\n",
    " \n",
    " or \n",
    " \n",
    " * $\\hat{r_{ui}} = \\frac{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) * r_{uj} }{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) }$\n",
    "  \n",
    "  \n",
    "### k-NN with Means Algorithm\n",
    "Predictions $\\hat{r_{ui}}$ are set with:\n",
    "\n",
    " * $\\hat{r_{ui}} = \\mu_u + \\frac{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) * (r_{vi} - \\mu_v) }{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) }$\n",
    " \n",
    " or \n",
    " \n",
    " * $\\hat{r_{ui}} = \\mu_i + \\frac{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) * (r_{uj} - \\mu_j) }{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) }$\n",
    "  \n",
    "  \n",
    "### k-NN with Z-Score Algorithm\n",
    "Predictions $\\hat{r_{ui}}$ are set with:\n",
    "\n",
    " * $\\hat{r_{ui}} = \\mu_u + \\sigma_u * \\frac{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) * (r_{vi} - \\mu_v)/\\sigma_v }{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) }$\n",
    " \n",
    " or \n",
    " \n",
    " * $\\hat{r_{ui}} = \\mu_i + \\sigma_u * \\frac{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) * (r_{uj} - \\mu_j)/\\sigma_j }{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) }$\n",
    "  \n",
    "  \n",
    "### k-NN with Baseline Algorithm\n",
    "Predictions $\\hat{r_{ui}}$ are set with:\n",
    "\n",
    " * $\\hat{r_{ui}} = b_{ui} + \\frac{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) * (r_{vi} - b_{vi}) }{\\sum_{v \\epsilon N_i^k(u)} sim(u,v) }$\n",
    " \n",
    " or \n",
    " \n",
    " * $\\hat{r_{ui}} = b_{ui} + \\frac{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) * (r_{uj} - b_{uj}) }{\\sum_{j \\epsilon N_u^k(i)} sim(i,j) }$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic KNN collaborative filtering\n",
    "rs = KNNBasic(\n",
    "    k = 40,    # Number of neighbors\n",
    "    min_k = 1, # Minimum number of neighbors to take into account for aggregation\n",
    "               # When not met the global average is used\n",
    "    #sim_options # A dictionary of options for the similarity measure\n",
    "    #,verbose = True\n",
    ")\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num \n",
    "    #,verbose=True\n",
    ")\n",
    "\n",
    "# KNN With Means collaborative filtering\n",
    "rs = KNNWithMeans(\n",
    "    k = 40,    # Number of neighbors\n",
    "    min_k = 1, # Minimum number of neighbors to take into account for aggregation\n",
    "               # When not met the global average is used\n",
    "    #sim_options # A dictionary of options for the similarity measure\n",
    "    #,verbose = True\n",
    ")\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num \n",
    "    #,verbose=True\n",
    ")\n",
    "\n",
    "# KNN with Z-Score collaborative filtering\n",
    "rs = KNNWithZScore(\n",
    "    k = 40,    # Number of neighbors\n",
    "    min_k = 1, # Minimum number of neighbors to take into account for aggregation\n",
    "               # When not met the global average is used\n",
    "    #sim_options # A dictionary of options for the similarity measure\n",
    "    #,verbose = True\n",
    ")\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num \n",
    "    #,verbose=True\n",
    ")\n",
    "\n",
    "# KNN With Means collaborative filtering\n",
    "rs = KNNBaseline(\n",
    "    k = 40,    # Number of neighbors\n",
    "    min_k = 1, # Minimum number of neighbors to take into account for aggregation\n",
    "               # When not met the global average is used\n",
    "    #sim_options # A dictionary of options for the similarity measure\n",
    "    #bsl_options # A dictionary containing the baseline scores\n",
    "    #,verbose = True\n",
    ")\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num \n",
    "    #,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization-based algorithms\n",
    "\n",
    "### SVD\n",
    "\n",
    "Intuitively, a matrix can be viewed as a collection of transformation from some basis. SVD is then the composition of these transformations. In 3D a matrix can be scaling, spinning, and rotations and then the SVD decomposition would be each of those individual components.\n",
    "\n",
    "The SVD algorithm has several variants, when baselines are not used then this algorithm is equivalent to Probabilistic Matrix Factorization\n",
    "\n",
    "The predictions $\\hat{r_{ui}}$ is set as:\n",
    " * $\\hat{r_{ui}} = \\mu + b_u + b_i + q_i^Tp_u$ \n",
    "\n",
    "If the user u is unknown, then the bias $b_u$ and the factors $p_u$ are assumed to be zero. The same applies for item i with $b_i$ and $q_i$ \n",
    "\n",
    "To estimate all the unknown values, the following regularized squared error is minimized.\n",
    "\n",
    " * $ \\sum_{r_{ui} \\epsilon R_{train}} (r_{ui} - \\hat{r_{ui}})^2 + \\lambda (b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)$\n",
    " \n",
    "This minimum is calculated using a straightforward gradient descent. \n",
    "\n",
    " * $b_u \\leftarrow b_u + \\gamma (e_{ui} - \\lambda b_u)$\n",
    " * $b_i \\leftarrow b_i + \\gamma (e_{ui} - \\lambda b_i)$\n",
    " * $p_u \\leftarrow p_u + \\gamma (e_{ui}*q_i - \\lambda p_u)$\n",
    " * $q_i \\leftarrow q_i + \\gamma (e_{ui}*p_u - \\lambda q_i)$\n",
    " \n",
    "Where $e_{ui} = r_{ui} \\hat{r_{ui}}$. These steps are performed over all the ratings of the itemset and repeated for each epoch by the parameter $n_epochs$. These baselines are initialized to 0. User and item factors are randomly initialized according to a normal distribution, which can can be updated with the init_mean and init_std_dev parameters. In addition, the learning rate $\\gamma$ and the regularization term $\\lambda$ can be manually adjusted. The defaults for the above parameters are 0.005 and 0.02 respectively.\n",
    "\n",
    "To change to the unbiased version of this algorithm predict\n",
    "\n",
    " * $\\hat{r_{ui}} = q_i^Tp_u$\n",
    "\n",
    "Which is the equivalent to Probabilistic Matrix Factorization\n",
    "\n",
    "### SVDpp\n",
    "\n",
    "The SVD plus plus algorithm, similar to the SVD algorithm but with implicit ratings. The predictions for $\\hat{r_{ui}}$ is set as\n",
    "\n",
    " * $\\hat{r_{ui}} = \\mu + b_u + b_i + q_i^T(p_u + |I_u|^{\\frac{-1}{2}} \\sum_{j \\epsilon I_u} y_j)$\n",
    " \n",
    "In this variation, the $y_j$ terms are a new set of items factors that capture implicit ratings. An implicit rating is defined as a user u rated an item j regardless of the rating value.\n",
    "\n",
    "Moreover if the user u is unknown, then the bias $b_u$ and the factors $p_u$ are assumed to be zero. The same applies for item i with $b_i$, $q_i$ and $y_i$.\n",
    "\n",
    "The baselines are initialized to 0. User and item factors are randomly initialized according to normal distribution. Altered with the parameters init_mean and init_std_dev. And control over learning rate and regularization terms with lr_all and reg_all.\n",
    "\n",
    "### NMF - Non-negative Matrix Factorization\n",
    "\n",
    "Intuitively, a matrix factorization into two matrices. All of these matrices the starting matrix and the two from the decomposition have all positive elements.\n",
    "\n",
    "A collaborative filtering algorithm based NMF, very close to SVD. The prediction $\\hat{r_{ui}}$ is set as:\n",
    "\n",
    " * $\\hat{r_{ui}} = q_i^Tp_u$\n",
    "\n",
    "Where the user and item factors are kept positive. This is optimized using (regularized) stochastic gradient descent (SGD) with step sizes that ensures non-negative values.\n",
    "\n",
    "For each step the SGD procedure, the factors f for user u and item i are updated as follows:\n",
    "\n",
    " * $p_{uf} \\leftarrow p_{uf} * \\frac{\\sum_{i \\epsilon I_u} q_{if} * r_{ui}}{\\sum_{i \\epsilon I_u} q_{if} * \\hat{r_{ui}} + \\lambda_u |I_u| p_{uf} }$\n",
    " \n",
    " * $q_{if} \\leftarrow q_{if} * \\frac{\\sum_{u \\epsilon U_i} p_{uf} * r_{ui}}{\\sum_{u \\epsilon U_i} p_{uf} * \\hat{r_{ui}} + \\lambda_i |U_i| q_{if} }$\n",
    " \n",
    "Where both lambda terms are for regularization. Note that this algorithm is highly dependent on the initial values. The user and item factors are uniformly initialized between init_low and init_high. A biased version is also available, this is done by setting the biased parameter to True.\n",
    "\n",
    " * $\\hat{r_{ui}} = \\mu + b_u + b_i + q_i^Tp_u$\n",
    " \n",
    "Furthermore, the baselines are optimized in the same way as in the SVD algorithm. While yeilding better accuracy, the biased version seems higly prone to overfitting thus reduction of factors may help, as well as, increasing the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the famous SVD algorithm.\n",
    "rs = SVD(\n",
    "    n_factors = 100, \n",
    "    n_epochs = 20,\n",
    "    biased = True,\n",
    "    init_mean = 0,\n",
    "    init_std_dev = 0.1,\n",
    "    lr_all = 0.005, # learnign rate for all parameters\n",
    "    reg_all = 0.02, # The regularization term for all parameters\n",
    "    #lr_bu, lr_bi, lr_pu, lr_qi, \n",
    "    #learning rate for bu, bi, pu, qi respectively\n",
    "    # reg_bu, reg_bi, reg_pu, reg_qi\n",
    "    # Regularization terms for bu, bi, pu, qi respectively\n",
    "    # random_state # used for random initialization\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rs = SVDpp(\n",
    "    n_factors = 100, \n",
    "    n_epochs = 20,\n",
    "    init_mean = 0,\n",
    "    init_std_dev = 0.1,\n",
    "    lr_all = 0.005, # learnign rate for all parameters\n",
    "    reg_all = 0.02, # The regularization term for all parameters\n",
    "    #lr_bu, lr_bi, lr_pu, lr_qi, \n",
    "    #learning rate for bu, bi, pu, qi respectively\n",
    "    # reg_bu, reg_bi, reg_pu, reg_qi\n",
    "    # Regularization terms for bu, bi, pu, qi respectively\n",
    "    # random_state # used for random initialization\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rs = NMF(\n",
    "    n_factors = 15, \n",
    "    n_epochs = 20,\n",
    "    biased = False,\n",
    "    # reg_pu, reg_qi, reg_bu, reg_bi\n",
    "    # regularization parameters\n",
    "    # lr_bu, lr_bi,\n",
    "    # learning rate parameters\n",
    "    # init_low, lower bound for random initialization\n",
    "    # init_high, higher bound for initialization of factors\n",
    "    # random_state # used for random initialization\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SlopeOne\n",
    "\n",
    "This algorithm can be easily used as a base line in performance. This is due to Slope One using a simpler form of linear regression when making predictions. Instead of $f(x) = ax + b$ it uses a simpler form $f(x) = x + b$. This version has been shown to be much more accurate than linear regression for some instances and takes much less storage.\n",
    "\n",
    "A simple collaborative filtering algorithm, the implementation of the SlopeOne algorithm in surprise follows a simple implementation. The prediction $\\hat{r_{ui}}$\n",
    "\n",
    " * $\\mu_u + \\frac{1}{|R_i(u)|} * \\sum_{j \\epsilon R_i(u)} dev(i,j)$\n",
    " \n",
    "The $R_i(u)$ is the set of relevant items, this is the set of items j rated by u that also have at least one common user with i. The dev(i,j) term is defined as the average difference between the ratings of i and j.\n",
    "\n",
    " * $d(i,j) = \\frac{1}{U_{ij}} * \\sum_{u \\epsilon U_{ij}} r_{ui} - r_{uj}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = SlopeOne()\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Clustering\n",
    "A collaborative filtering algorithm based on co-clustering. This works by assigning users and items to some clusters $C_u$ and $C_i$ and some co-clusters $C_{ui}$. The prediction $\\hat{r_{ui}}$ is set with:\n",
    "\n",
    " * $\\hat{r_{ui}} = \\bar{C_{ui}} + (\\mu_u - \\bar{C_u}) + (\\mu_i - \\bar{C_i})$\n",
    " \n",
    " The $\\bar{C_{ui}}$ is the average rating of co-clustering $C_{ui}$. $\\bar{C_u}$ is the average rating of u's cluster, and $\\bar{C_i}$ is the average rating of i's cluster. If the user is unknown, the prediction is $\\hat{r_{ui}} = \\mu_i$. If the item is unknown, the prediction is $\\hat{r_{ui}} = \\mu_u$. If both the user and item are unknown, the prediction is just $\\hat{r_{ui}} = \\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = CoClustering(\n",
    "    n_cltr_u = 3, # number of user clusters\n",
    "    n_cltr_i = 3, # number of item clusters\n",
    "    n_epochs = 20, #Number of iterations of optimization loop\n",
    "    #random_state,\n",
    "    #verbose\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    data, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"KNNBasic:\")\n",
    "rs = KNNBasic(\n",
    "    k = 40,    # Number of neighbors\n",
    "    min_k = 1, # Minimum number of neighbors to take into account for aggregation\n",
    "               # When not met the global average is used\n",
    "    #sim_options # A dictionary of options for the similarity measure\n",
    "    #,verbose = True\n",
    ")\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    df, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num \n",
    "    #,verbose=True\n",
    ")\n",
    "print(cval)\n",
    "print(\"SVD:\")\n",
    "rs = SVD(\n",
    "    n_factors = 100, \n",
    "    n_epochs = 20,\n",
    "    biased = True,\n",
    "    init_mean = 0,\n",
    "    init_std_dev = 0.1,\n",
    "    lr_all = 0.005, # learnign rate for all parameters\n",
    "    reg_all = 0.02, # The regularization term for all parameters\n",
    "    #lr_bu, lr_bi, lr_pu, lr_qi, \n",
    "    #learning rate for bu, bi, pu, qi respectively\n",
    "    # reg_bu, reg_bi, reg_pu, reg_qi\n",
    "    # Regularization terms for bu, bi, pu, qi respectively\n",
    "    # random_state # used for random initialization\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    df, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")\n",
    "print(cval)\n",
    "print(\"SVDpp:\")\n",
    "rs = SVDpp(\n",
    "    n_factors = 100, \n",
    "    n_epochs = 20,\n",
    "    init_mean = 0,\n",
    "    init_std_dev = 0.1,\n",
    "    lr_all = 0.005, # learnign rate for all parameters\n",
    "    reg_all = 0.02, # The regularization term for all parameters\n",
    "    #lr_bu, lr_bi, lr_pu, lr_qi, \n",
    "    #learning rate for bu, bi, pu, qi respectively\n",
    "    # reg_bu, reg_bi, reg_pu, reg_qi\n",
    "    # Regularization terms for bu, bi, pu, qi respectively\n",
    "    # random_state # used for random initialization\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    df, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")\n",
    "print(cval)\n",
    "\n",
    "print(\"CoClustering:\")\n",
    "rs = CoClustering(\n",
    "    n_cltr_u = 3, # number of user clusters\n",
    "    n_cltr_i = 3, # number of item clusters\n",
    "    n_epochs = 20, #Number of iterations of optimization loop\n",
    "    #random_state,\n",
    "    #verbose\n",
    ")\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    df, \n",
    "    measures = measure_set, \n",
    "    cv = cv_num, \n",
    "    verbose=True\n",
    ")\n",
    "print(cval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Slope One:\")\n",
    "rs = SlopeOne()\n",
    "\n",
    "cval = cross_validate(\n",
    "    rs, \n",
    "    df, \n",
    "    measures = measure_set, \n",
    "    cv = 10, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}